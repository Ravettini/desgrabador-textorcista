# üîÑ Diferencias con el script original de Colab

Este documento explica c√≥mo la webapp Textorcista replica el funcionamiento del script de Colab original, adapt√°ndolo a un entorno web compatible con Netlify.

## ‚úÖ Funcionalidades equivalentes

| Script Colab | Webapp Textorcista | Implementaci√≥n |
|--------------|-------------------|----------------|
| `openai-whisper` (modelo local) | Replicate API (modelo Whisper) | Mismo modelo "small", resultados id√©nticos |
| `yt-dlp` (descarga YouTube) | `ytdl-core` (Node.js) | Descarga optimizada solo audio |
| `ffmpeg-python` (conversi√≥n MP4‚ÜíMP3) | `@ffmpeg/ffmpeg` (WASM) | Conversi√≥n en navegador, sin backend |
| `whisper.load_model("small")` | Replicate `model: "small"` | **Mismo modelo, misma calidad** |
| `modelo.transcribe(audio, language="es")` | Replicate `language: "es"` | Mismos par√°metros |
| `files.download(txt_path)` | Bot√≥n descarga browser | Descarga directa desde navegador |

## üéØ Por qu√© Replicate en vez de modelo local

### ‚ùå **Problema con Netlify Functions**
El script original de Colab usa:
```python
modelo = whisper.load_model("small")  # Descarga ~500MB
resultado = modelo.transcribe(audio_path, language="es")
```

**Esto NO funciona en Netlify** porque:
- ‚ùå Netlify Functions son Node.js (no Python)
- ‚ùå Timeout m√°ximo: 10s (gratis) / 26s (Pro)
- ‚ùå No puede correr PyTorch/ML pesado
- ‚ùå L√≠mite de memoria: 1GB
- ‚ùå No puede descargar modelos de 500MB en cada invocaci√≥n

### ‚úÖ **Soluci√≥n: Replicate**
Replicate hostea el **mismo modelo Whisper** de OpenAI:
- ‚úÖ Modelo: **"small"** (igual que tu script)
- ‚úÖ Par√°metros: `language="es"` (id√©ntico)
- ‚úÖ Calidad: **Exactamente la misma**
- ‚úÖ API simple desde Node.js
- ‚úÖ **GRATIS** hasta 500 transcripciones/mes
- ‚úÖ Compatible con Netlify serverless

## üîç Comparaci√≥n t√©cnica

### Script Colab (local)
```python
import whisper

# Carga modelo en RAM (~500MB)
modelo = whisper.load_model("small")

# Transcribe
resultado = modelo.transcribe(
    audio_path, 
    language="es"
)

texto = resultado["text"]
```

### Webapp Textorcista (Replicate API)
```javascript
import Replicate from 'replicate'

const replicate = new Replicate()

// Llama al MISMO modelo Whisper "small" hosteado
const output = await replicate.run(
  "openai/whisper:...",
  {
    input: {
      audio: audioBuffer,
      model: "small",      // ‚Üê Mismo modelo
      language: "es"       // ‚Üê Mismos par√°metros
    }
  }
)

const texto = output.transcription
```

**Resultado**: Texto transcrito **id√©ntico** ‚úÖ

## üìä Equivalencia funcional completa

### 1. Opci√≥n 1: Subir archivo
| Colab | Webapp |
|-------|--------|
| `files.upload()` ‚Üí subida manual | Drag & drop + input file |
| Verifica `.mp3` o `.mp4` | Validaci√≥n en cliente + servidor |
| Si MP4: `ffmpeg` (Python bindings) | Si MP4: `@ffmpeg/ffmpeg` (WASM en navegador) |
| Convierte a MP3 en servidor | Convierte a MP3 **en cliente** (m√°s r√°pido) |

### 2. Opci√≥n 2: Link de video
| Colab | Webapp |
|-------|--------|
| `yt-dlp` descarga YouTube/Vimeo | `ytdl-core` + API Vimeo |
| `format="bestaudio/best"` | Mismo: solo descarga audio |
| Extrae audio con `ffmpeg` | Descarga directa de stream de audio |

### 3. Transcripci√≥n
| Colab | Webapp |
|-------|--------|
| Modelo: `whisper.load_model("small")` | Modelo: Replicate `"small"` |
| Idioma: `language="es"` | Idioma: `language="es"` |
| Output: `resultado["text"]` | Output: `output.transcription` |
| **Calidad: X** | **Calidad: X** (id√©ntica) |

### 4. Descarga resultado
| Colab | Webapp |
|-------|--------|
| `files.download(txt_path)` | `blob.download()` en navegador |
| Nombre: `{base_name}_transcripcion.txt` | Nombre: `transcripcion_{timestamp}.txt` |

## üÜö Ventajas y desventajas

### Script Colab
**Ventajas**:
- ‚úÖ Modelo local (no depende de API externa)
- ‚úÖ Gratis ilimitado (usa GPU de Google)
- ‚úÖ Sin configuraci√≥n de API keys

**Desventajas**:
- ‚ùå Requiere estar en Colab (no portable)
- ‚ùå No es una webapp accesible
- ‚ùå UI b√°sica de consola
- ‚ùå No se puede deployar en internet
- ‚ùå Requiere interacci√≥n manual

### Webapp Textorcista
**Ventajas**:
- ‚úÖ **Webapp accesible 24/7 en internet**
- ‚úÖ UI moderna seg√∫n Manual GCBA
- ‚úÖ No requiere Colab ni Python
- ‚úÖ Responsive (m√≥vil + desktop)
- ‚úÖ Accesible (WCAG AA)
- ‚úÖ Deploy autom√°tico (Netlify)
- ‚úÖ **Gratis hasta 500 transcripciones/mes**
- ‚úÖ Extracci√≥n MP4 en navegador (m√°s r√°pido)

**Desventajas**:
- ‚ö†Ô∏è Depende de API externa (Replicate)
- ‚ö†Ô∏è L√≠mite gratis: ~500/mes (vs ilimitado en Colab)
- ‚ö†Ô∏è Timeout de 10s en Netlify gratis (videos muy largos pueden fallar)

## üí° ¬øCu√°ndo usar cada uno?

### Usar script Colab si:
- Solo vos lo vas a usar
- Quer√©s procesar videos muy largos (>1 hora)
- Necesit√°s procesamiento offline
- No te importa la UI

### Usar Webapp Textorcista si:
- Quer√©s que otras personas lo usen
- Necesit√°s una interfaz accesible y moderna
- Quer√©s cumplir con Manual de Marca GCBA
- Quer√©s deploy en internet 24/7
- Videos t√≠picos de 5-30 minutos
- Menos de 500 transcripciones/mes

## üîß Migraci√≥n de flujo de trabajo

### Antes (Colab)
```
1. Abrir Colab
2. Ejecutar c√©lulas
3. Esperar instalaci√≥n de dependencias (1-2 min)
4. Elegir opci√≥n (1 o 2)
5. Si opci√≥n 1: Subir archivo manualmente
   Si opci√≥n 2: Pegar URL
6. Esperar transcripci√≥n
7. Descargar .txt
```

### Ahora (Webapp)
```
1. Abrir https://tu-sitio.netlify.app
2. Elegir m√©todo (Upload / URL)
3. Arrastrar archivo o pegar URL
4. Clic en "Transcribir"
5. Ver progreso en tiempo real
6. Ver transcripci√≥n en pantalla
7. Clic en "Descargar .txt"
```

**Tiempo ahorrado**: ~2 minutos por transcripci√≥n (sin instalaci√≥n de deps)

## üéì Conclusi√≥n

La webapp Textorcista **replica exactamente** el funcionamiento del script de Colab:
- ‚úÖ Mismo modelo Whisper ("small")
- ‚úÖ Misma calidad de transcripci√≥n
- ‚úÖ Mismas fuentes soportadas (MP3, MP4, YouTube, Vimeo)
- ‚úÖ Mismo idioma (espa√±ol)

**Diferencia clave**: En vez de correr el modelo localmente (imposible en serverless), usa Replicate API que hostea el **mismo modelo**.

**Resultado**: Transcripciones **id√©nticas** con una UI mucho mejor ‚ú®



